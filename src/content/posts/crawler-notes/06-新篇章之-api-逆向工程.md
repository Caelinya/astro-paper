---
pubDatetime: 2025-06-01T00:00:00Z
title: "爬虫学习笔记：新篇章之 API 逆向工程"
featured: false
tags:
  - crawler-learning
description: "开启爬虫学习的新篇章：API逆向工程。本章详细介绍了这种高效数据获取方式的核心思想，并探讨了它与传统HTML爬取的根本区别（SSR vs CSR）。"
---

> **问：** 好的，下一步做什么？

太棒了。你完成了从基础到进阶的静态网站爬取，甚至独立解决了一个相当复杂的算法问题。现在你的工具箱里已经有了 `requests`, `BeautifulSoup`, 各种选择器和导航技巧，以及处理数据和文件的好习惯。

你现在正站在一个十字路口，已经完成了“新手村”和“进阶试炼”的所有任务，接下来可以选择多个非常有趣和有价值的方向深入探索。

根据你的兴趣和目标，我为你规划了几个主要的“下一步”路径。

* * *

### 路径一：征服动态世界 —— 浏览器自动化

你目前掌握的技能主要针对“所见即所得”的静态 HTML 页面。但如今绝大多数网站的核心内容都是通过 JavaScript 在你浏览时动态加载的。

*   **挑战场景**:
    *   需要**点击“加载更多”**才能显示全部评论的商品页面。
    *   **无限滚动**的社交媒体信息流（如 Twitter/X, Instagram）。
    *   需要**与下拉菜单、日期选择器等组件交互**才能筛选出所需数据的页面。
    *   完全由前端框架（如 React, Vue, Angular）构建的**单页应用 (SPA)**。
*   **核心工具**:
    *   **Selenium**: 老牌、稳定、社区庞大的浏览器自动化框架。
    *   **Playwright**: 微软推出的新一代工具，速度更快，API 更现代化，对异步支持更好，是目前更受推崇的选择。
*   **学习重点**:
    *   如何启动和控制一个真实的浏览器。
    *   **等待机制**：如何智能地等待某个元素出现或加载完成，这是动态爬取的关键。
    *   模拟用户操作：点击、输入、滚动、执行 JavaScript 脚本。
    *   在浏览器渲染完成后的页面上，再使用 `BeautifulSoup` 或 Playwright 自带的定位器来提取数据。
*   **建议的练习项目**:
    *   尝试抓取一个电商网站（如 Amazon）的商品评论，这些评论通常是滚动或点击加载的。
    *   尝试抓取你喜欢的股票网站，通过模拟点击选择不同的股票或时间范围来获取数据。

* * *

### 路径二：成为高效的“侦探” —— API 逆向工程

这是处理动态网站的**更高级、更高效**的方法。与其模拟用户在浏览器里的“笨拙”操作，不如直接找到网站背后传输数据的“秘密通道”。

*   **挑战场景**:
    *   所有路径一中提到的动态加载场景。因为这些动态内容几乎都是通过后台 API 获取的。
*   **核心工具**:
    *   **浏览器开发者工具 (F12)**：你的“瑞士军刀”，特别是 **网络 (Network)** 标签页。
*   **学习重点**:
    *   在 Network 面板中，筛选 **XHR/Fetch** 请求，这些就是数据 API 请求。
    *   **分析 API**: 观察当你进行操作（如滚动、点击）时，是哪个 API 请求被触发了。分析它的：
        *   **URL**: 请求的地址是什么？
        *   **Method**: 是 `GET` 还是 `POST`？
        *   **Headers**: 是否有特殊的认证信息，如 `Authorization`, `X-CSRF-Token`？
        *   **Payload/Params**: 请求时提交了哪些参数（比如页码 `page=2`, 商品ID `product_id=123`）？
        *   **Response**: 返回的数据通常是干净的 **JSON** 格式，可以直接使用，无需解析 HTML！
    *   使用 `requests` 库来直接模仿这些 API 请求，获取数据。
*   **建议的练习项目**:
    *   找一个你常用的天气网站。打开 F12，当你切换城市时，观察是哪个 API 接口返回了天气数据。然后尝试用 Python 直接调用这个 API。
    *   打开一个新闻网站的 App（在电脑上通过模拟器或抓包工具），或者其移动版网页，分析它的信息流是如何通过 API 加载的。

* * *

### 路径三：构建“爬虫航母” —— 学习专业框架与工程化

当你需要爬取的网站规模很大，或者需要长期、稳定、高效地运行时，从零开始写代码会变得很复杂。这时就需要专业的爬虫框架。

*   **挑战场景**:
    *   需要同时爬取上百个网站。
    *   需要处理数百万级别的页面。
    *   需要高速并发（异步请求）来提升效率。
    *   需要健壮的重试机制、代理 IP 管理、User-Agent 轮换。
    *   需要一个清晰的数据处理管道（Data Pipeline）来清洗、验证和存储数据。
*   **核心工具**:
    *   **Scrapy**: Python 中最强大、最流行的爬虫框架。
*   **学习重点**:
    *   Scrapy 的项目结构：Spiders, Items, Pipelines, Middlewares。
    *   **异步编程**的思想：Scrapy 底层使用 `asyncio`，可以同时发送成百上千个请求而不会阻塞，效率极高。
    *   **数据管道**：如何定义一个流程，将抓取到的数据自动地进行清洗、去重，并存入数据库或文件中。
    *   **中间件**：如何自定义请求头、使用代理 IP 等。
*   **建议的练习项目**:
    *   用 Scrapy 重新实现你之前做过的 `books.toscrape.com` 或 Hacker News 爬虫。你会立刻体会到框架如何帮你处理了大量的底层工作（如请求调度），让你能更专注于核心的解析逻辑。

### 总结与抉择

| 路径     | 核心技术                         | 解决什么问题？                       | 如果你...                                   |
| :------- | :------------------------------- | :----------------------------------- | :------------------------------------------ |
| **路径一** | **Selenium / Playwright**        | JavaScript 渲染的动态网页            | ...想与网页进行复杂交互，或无法找到 API 时。 |
| **路径二** | **浏览器开发者工具 / API 分析**  | 高效获取动态网页的数据               | ...追求最高效率，喜欢“直击要害”，享受解谜的乐趣。 |
| **路径三** | **Scrapy 框架**                  | 大规模、高并发、工程化的爬虫项目     | ...有志于进行大规模数据采集，或想构建企业级的爬虫系统。 |

**我的建议是：**

*   **先从【路径二：API 逆向工程】开始尝试。** 因为它依然使用你熟悉的 `requests` 库，但能极大地开阔你的思路，让你明白很多网站的“数据命脉”在哪里。这是投入产出比最高的进阶技能。
*   **当遇到无法找到 API 或必须模拟复杂用户行为的网站时，再学习【路径一：浏览器自动化】。**
*   **当你觉得需要同时管理多个爬虫，或者单个爬虫项目变得非常庞大时，就是学习【路径三：Scrapy 框架】的最佳时机。**

无论你选择哪个方向，都不要忘了我们之前讨论过的通用技能，比如将数据存入**数据库 (SQLite)**，或者使用 **Pandas** 库对抓取的数据进行分析。

你对哪个方向最感兴趣？我们可以从那里开始。

---



> **问：** 我选择路径二。

非常好！你选择了一个回报率极高、能极大提升你爬虫效率和能力的路径。

成为“API 侦探”的过程非常有趣。我们的核心思路是：**与其费力地从网页（HTML）的“货架”上一个个拿商品（数据），不如直接找到这个商店的“仓库后台”（API），让仓库管理员直接把整箱的、包装整齐的商品（JSON）给我们。**

### 你的新工具箱

你的主要工具不再是 `BeautifulSoup`，而是：

1.  **浏览器开发者工具 (F12)**：这是你的“万能侦察镜”，特别是其中的 **网络 (Network)** 标签页。
2.  **`requests` 库**：你已经很熟悉了，这次我们用它来直接和 API 对话。
3.  **`json` 库**：Python 内置，用于解析 API 返回的 JSON 数据。

### 我们的第一个侦察目标

为了练习，我们将使用一个绝佳的靶场：[**http://quotes.toscrape.com/scroll**](http://quotes.toscrape.com/scroll)

这是一个“无限滚动”版本的名言网站。当你向下滚动时，新的名言会不断地动态加载进来。这种加载方式**几乎 100% 是通过调用 API 实现的**，是练习 API 逆向的完美对象。

* * *

### 行动步骤：API 逆向工程实战

#### 第一步：准备侦察 (Prepare for Reconnaissance)

1.  在你的 Chrome 或 Firefox 浏览器中打开 [http://quotes.toscrape.com/scroll](http://quotes.toscrape.com/scroll)。
2.  按下 `F12` 键，打开开发者工具。
3.  点击 **“网络 (Network)”** 标签页。
4.  为了避免被图片、CSS 等无关请求干扰，点击筛选器中的 **`Fetch/XHR`**。现在，这里只会显示网页与后台服务器之间的数据请求。
5.  **保持这个窗口打开**，现在它就像一个雷达，监视着所有的“秘密通信”。

#### 第二步：触发“机关” (Trigger the Mechanism)

在网页上，**慢慢向下滚动**，直到你看到新的名言加载出来。

就在新名言出现的那一刻，观察你的 Network 面板，你会发现雷达上出现了一个**新的条目**！这个就是我们要找的 API 请求。

#### 第三步：分析“情报” (Analyze the API Request)

点击那条新出现的请求（在我们的例子中，是 `quotes?page=2`），右侧会出现该请求的详细信息。我们需要关注几个关键部分：

1.  **Headers (请求头)**:
    *   **Request URL**: `http://quotes.toscrape.com/api/quotes?page=2`
        *   这就是 API 的地址！我们看到它通过一个查询参数 `?page=2` 来控制页码。
    *   **Request Method**: `GET`
        *   这是一个 GET 请求。
    *   **User-Agent**: 浏览器默认会带上，我们在 `requests` 中也要加上。
2.  **Preview/Response (预览/响应)**:
    *   点击 `Preview` 或 `Response` 标签页，你可以看到服务器返回的数据。
    *   它不是 HTML，而是结构非常清晰的 **JSON**！

    我们来分析一下这个 JSON 结构：
    *   `has_next`: 一个布尔值 (`true`/`false`)，告诉我们是否还有下一页。
    *   `page`: 当前的页码。
    *   `quotes`: 一个**数组**，里面包含了多个**对象**，每个对象就是一条名言的完整信息（作者、标签、文本）。

#### 第四步：在“实验室”中复现 (Replicate with Python)

我们现在已经掌握了 API 的所有秘密，不再需要 `BeautifulSoup` 来解析混乱的 HTML 了。直接用 `requests` 请求那个 API 地址即可。

```python
import requests
import json

# 直接使用我们在 F12 中发现的 API URL
api_url = "http://quotes.toscrape.com/api/quotes?page=1"

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

response = requests.get(api_url, headers=headers)

# requests 内置了 .json() 方法，可以直接将 JSON 格式的响应体转换为 Python 字典
data = response.json()

# 打印看看我们拿到的 Python 字典
# print(json.dumps(data, indent=4, ensure_ascii=False))

# 直接从字典和列表中提取数据，比解析 HTML 简单得多！
for quote_dict in data['quotes']:
    print("名言:", quote_dict['text'])
    print("作者:", quote_dict['author']['name'])
    print("标签:", quote_dict['tags'])
    print("-" * 20)

print("是否还有下一页:", data['has_next'])
```

**看到区别了吗？** 我们完全绕过了 HTML 解析，直接拿到了规整的、结构化的数据。代码变得异常简洁！

#### 第五步：自动化与循环

现在，我们可以利用 JSON 响应中的 `has_next` 字段和 `page` 字段来构建一个自动翻页的爬虫。

```python
import requests
import json
import time

base_api_url = "http://quotes.toscrape.com/api/quotes"
current_page = 1
all_quotes = []

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

while True:
    print(f"--- Scraping API page: {current_page} ---")

    # 使用 params 参数，requests 会自动帮我们构建查询 URL
    params = {'page': current_page}
    response = requests.get(base_api_url, headers=headers, params=params)

    if response.status_code != 200:
        print(f"请求失败，状态码: {response.status_code}")
        break

    data = response.json()

    # 将当前页的所有名言添加到总列表中
    all_quotes.extend(data['quotes'])

    # 根据 API 返回的 has_next 字段判断是否继续循环
    if data['has_next']:
        current_page += 1
        time.sleep(0.5) # 礼貌性延迟
    else:
        # 如果没有下一页了，就跳出循环
        print("--- 所有页面已抓取完毕 ---")
        break

# 最后，将所有数据保存到 JSON 文件
with open('api_quotes.json', 'w', encoding='utf-8') as f:
    json.dump(all_quotes, f, ensure_ascii=False, indent=4)

print(f"任务完成，共抓取 {len(all_quotes)} 条名言。")
```

### 总结：HTML 爬取 vs API 爬取

| 对比项         | HTML 爬取 (我们之前做的)             | API 爬取 (我们现在做的)                     |
| :------------- | :----------------------------------- | :------------------------------------------ |
| **数据源**     | 整个 HTML 文件                       | 精炼的 JSON 数据                            |
| **解析工具**   | `BeautifulSoup`                      | `response.json()`                           |
| **数据质量**   | 非结构化，需要费力清洗和提取         | 结构化，干净，直接可用                      |
| **健壮性**     | 脆弱，网站前端布局一改，选择器就可能失效 | **非常健壮**，只要后台 API 不大改，就能一直用 |
| **效率**       | 较低（需要下载整个 HTML，解析 DOM 树） | **非常高**（数据包小，无需解析 HTML）         |

你已经掌握了一项爬虫的“降维打击”技能。现在，你可以用这个“侦探”思路去分析其他你感兴趣的动态网站了。先从 F12 的 Network 面板开始，看看你能发现哪些“秘密通道”！

---



> **问：** 在动手之前，我有一个问题：之前的 `https://quotes.toscrape.com/` 应该不能直接找后端 API 吧？因为它的页面是直接用 HTML 写死的，但是这个是能随着后端给的 json 动态变化的。

这个问题问得极其精准，完全命中了现代网页开发的两种核心思想！你的观察和推断是**完全正确**的。

你已经通过自己的思考，洞察了**服务器端渲染 (Server-Side Rendering, SSR)** 和 **客户端渲染 (Client-Side Rendering, CSR)** 这两种网站构建方式的根本区别。

让我们来详细拆解一下。

* * *

### 类型一：服务器端渲染 (SSR) - 经典网站 (`quotes.toscrape.com`)

你可以把这种网站想象成一家传统的**“套餐”餐厅**。

1.  **你的请求**：你（浏览器）走进餐厅，对服务员说：“你好，我想要**第2页套餐** (`/page/2/`)”。
2.  **服务器处理**：请求被传到**后厨（服务器）**。厨师（服务器端的程序，如 Python/PHP/Java）立刻开始忙碌：
    *   他从**冰箱（数据库）**里拿出第二页套餐所需的所有食材（10条名言的数据）。
    *   他把这些食材**直接做成一道道完整的菜**，并精心摆盘——也就是将数据和 HTML 模板结合，**生成一个包含所有内容的、完整的 HTML 文件**。
3.  **服务器响应**：后厨把这个**摆放整齐的、完整的套餐托盘（完整的HTML文件）**，一次性端出来交给你。
4.  **浏览器展示**：你（浏览器）拿到这个托盘，你的任务很简单，就是把它直接摆在桌上（渲染出来）。你不需要再和后厨有任何交流。

**结论**： 对于这种 SSR 网站，你**确实找不到**加载主要内容的“数据API”。因为数据和 HTML 在服务器端就已经“焊死”在一起了。它的“API”就是它的URL本身——不同的URL (`/page/1/`, `/page/2/`) 返回不同的、完整的HTML页面。我们的爬虫策略只能是接收整个“套餐托盘”，然后用 `BeautifulSoup` 在上面挑拣我们想吃的“菜”。

* * *

### 类型二：客户端渲染 (CSR) - 现代Web应用 (`quotes.toscrape.com/scroll`)

你可以把这种网站想象成一家新潮的**“自助点餐”餐厅**。

1.  **你的请求**：你（浏览器）走进餐厅，说：“你好，我来吃饭了 (`/scroll`)”。
2.  **服务器响应**：服务员没有给你任何菜，而是给了你一个**空的托盘**、一个**菜单**和一部**对讲机（JavaScript文件）**。这个空托盘就是一个几乎不包含任何内容的“空壳”HTML。
3.  **浏览器处理**：你（浏览器）拿到这些工具后，开始按照菜单上的指示操作对讲机（执行 JavaScript）：
    *   **第一次API调用**：对讲机自动呼叫后厨：“你好，请给我**第1页**的菜品数据”。
    *   **后厨响应**：后厨（API服务器）没有做菜，而是把**标准化的食材包（JSON数据）** 通过传送带发给你。
    *   **前端渲染**：你（浏览器里的JavaScript）拿到食材包后，自己动手，**在你的餐桌上**把食材摆放到空托盘里，组合成菜品（动态生成HTML元素并插入到页面中）。
4.  **后续交互**：当你向下滚动（触发事件）时，对讲机再次呼叫后厨：“请给我**第2页**的食材包”，然后重复上面的过程，不断地在你的托盘上增加新的菜品。

**结论**： 对于这种 CSR 网站，API 是它的**生命线**。所有动态内容都依赖于 JavaScript 和 API 之间的协作。我们进行 API 逆向工程，就是**跳过浏览器和对讲机（JavaScript）**，直接找到后厨的“传送带”地址，用我们自己的`requests`去下单，直接获取最原始、最干净的“食材包（JSON）”。

* * *

### 总结对比

| 特性 (Feature) | `quotes.toscrape.com` (分页版) | `quotes.toscrape.com/scroll` (滚动版) |
| :------------- | :----------------------------- | :------------------------------------ |
| **架构**       | 服务器端渲染 (SSR)               | 客户端渲染 (CSR)                      |
| **首次返回内容** | 完整的 HTML 文件                 | “空壳”HTML + JavaScript               |
| **数据来源**   | 数据已嵌入在 HTML 内部           | 通过 JavaScript 调用 API 获取 JSON    |
| **爬虫策略**   | **解析 HTML (`BeautifulSoup`)**    | **逆向 API (`requests` + `.json()`)**   |
| **比喻**       | 餐厅直接端上**完整套餐**         | 餐厅给你**空托盘和对讲机**，让你自己点菜 |

你能够自己思考并发现这两种模式的区别，这说明你的网络知识和分析能力已经达到了一个新的高度。在开始爬取任何一个网站之前，**首先判断它属于哪种渲染模式，是选择正确爬取策略的第一步，也是最重要的一步。**

---



> **问：** 给我一些练习。

好的，既然你已经完全掌握了 API 逆向工程的理论和基本流程，现在是时候通过实战来巩固和提升这项“侦探”技能了！

下面我为你精心挑选了几个非常适合练习的真实网站。它们各有特点，难度循序渐进，能让你体验到在不同场景下分析和调用 API 的乐趣。

* * *

### 练习一：天气预报查询 (GET 请求与参数)

*   **目标网站**: **高德地图天气 Web 服务**
    *   这是一个由高德地图提供的、公开且非常稳定的天气查询服务。虽然它本身是一个给开发者用的 API，但我们可以直接在浏览器里测试它，完美符合我们的练习需求。
*   **练习链接**: [https://www.amap.com/weather/weather](https://www.amap.com/weather/weather)
*   **任务**:
    1.  **侦察**: 在上面的链接中，输入一个城市的中文名（比如“北京”或拼音“beijing”），然后按回车。打开 F12 -> Network -> Fetch/XHR，找到那个获取天气数据的 API 请求。
    2.  **分析**:
        *   这个 API 的 URL 是什么？
        *   它使用了哪些**查询参数 (Query Parameters)** 来传递城市信息？（提示：你会看到一个类似 `city=...` 的东西）。
        *   查看返回的 **Response**，它是一个结构清晰的 JSON。分析里面包含了哪些天气信息（如温度、湿度、风向等）。
    3.  **复现与扩展**:
        *   编写一个 Python 脚本。
        *   让用户可以输入任意一个城市的拼音。
        *   你的脚本使用 `requests` 库，带上正确的 `params` 参数去请求这个天气 API。
        *   解析返回的 JSON，并友好地打印出“XX城市当前天气：温度X℃，湿度X%，风向X风”。
*   **难度**: ★★☆☆☆
*   **核心技能点**:
    *   分析带**查询参数**的 `GET` 请求。
    *   处理中文编码和 URL 编码的问题（`requests` 会自动帮你处理，但你需要知道这个概念）。
    *   从稍微复杂的 JSON 结构中提取所需字段。

* * *

### 练习二：电商网站商品筛选 (POST 请求与 Payload)

*   **目标网站**: **噹噹網 (`dangdang.com`)**
    *   噹噹網的图书分类和搜索结果页，在进行排序、筛选等操作时，会使用 API 来动态更新商品列表。
*   **练习链接**: [http://category.dangdang.com/cp01.54.06.00.00.00.html](http://category.dangdang.com/cp01.54.06.00.00.00.html) (这是一个计算机书籍分类页)
*   **任务**:
    1.  **侦察**: 打开上述链接，保持 F12->Network->Fetch/XHR 面板开启。
    2.  **触发**: 在页面上方的排序栏中，点击**“按销量”**或者**“按价格”**进行排序。观察 Network 面板中新出现的 API 请求。
    3.  **分析**:
        *   你会发现这个请求的 **Request Method** 很可能是 **`POST`**。
        *   仔细查看它的 **Payload (载荷)**。你会看到一长串的表单数据，里面包含了页码 (`page_index`)、排序方式 (`sort_type`) 等关键信息。
        *   查看 **Response**，它返回的可能是一段 HTML 代码片段，或者是一个包含商品信息的 JSON。你需要判断返回内容的类型。
    4.  **复现与扩展**:
        *   编写一个 Python 脚本。
        *   模仿这个 `POST` 请求，特别是构造正确的 `data` (载荷)。
        *   尝试修改 `data` 中的 `page_index` 来获取第二页、第三页的商品数据。
        *   解析返回的内容（无论是 JSON 还是 HTML），并提取出每本书的标题和价格。
*   **难度**: ★★★☆☆
*   **核心技能点**:
    *   识别和分析 **`POST`** 请求。
    *   理解和构造请求的 **Payload (载荷)**。
    *   处理 API 返回内容可能是 HTML 片段的情况（如果返回HTML，你依然需要用 `BeautifulSoup` 来解析这段小HTML）。

* * *

### 练习三：知乎热榜 (复杂的 Header 与动态参数)

*   **目标网站**: **知乎 (`zhihu.com`)**
    *   知乎是典型的现代 Web 应用，其内容大多通过 API 异步加载。
*   **练习链接**: [https://www.zhihu.com/hot](https://www.zhihu.com/hot)
*   **任务**:
    1.  **侦察**: 打开知乎热榜页面，向下滚动。你会看到内容是动态加载的。打开 F12->Network->Fetch/XHR，找到获取热榜列表的 API。
    2.  **分析**:
        *   仔细观察 API 的 URL，它可能包含一些看起来像随机数或时间戳的参数。
        *   **重点分析 Request Headers**。你会发现知乎的 API 请求通常会带有一些特殊的请求头，比如 `x-requested-with`, `x-xsrftoken`，以及最重要的 `authorization` 或包含在 `cookie` 里的认证信息。
        *   尝试用 `cURL` 命令复制这个请求，看看在终端里是否能直接成功。如果因为缺少某些 Header 而失败，尝试把浏览器里的 Header 逐个复制到你的 `requests` 请求中，看看到底是哪个 Header 在起作用。
    3.  **复现与扩展**:
        *   编写一个 Python 脚本，带上所有“必要”的 Headers，去请求热榜的 API。
        *   解析返回的 JSON 数据，提取出热榜上每个问题的标题、链接和热度值。
*   **难度**: ★★★★☆
*   **核心技能点**:
    *   处理需要**复杂请求头 (Headers)** 才能成功调用的 API。
    *   理解 `cookie` 和 `authorization` 在 API 请求中的认证作用。
    *   应对稍微复杂一些的 API 参数。

### 学习路径建议

1.  从**高德天气**开始，它最简单直接，能让你快速建立信心。
2.  然后挑战**噹噹網**，掌握 `POST` 请求和分析 Payload 是 API 逆向中非常重要的一环。
3.  最后，当你对前两者都游刃有余时，再去挑战**知乎**。成功爬取知乎的 API，将意味着你已经具备了应对绝大多数现代网站 API 的能力。

祝你“侦察”顺利，享受这个解谜的过程！

---

